**11.relu为何好过sigmoid和tanh？**

先看sigmoid、tanh和RelU的函数图：

![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1513007001_317.png)

第一，采用sigmoid等函数，算激活函数时（指数运算），计算量大，反向传播求误差梯度时，求导涉及除法和指数运算，计算量相对大，而采用Relu激活函数，整个过程的计算量节省很多。

第二，对于深层网络，sigmoid函数反向传播时，很容易就会出现梯度消失的情况（在sigmoid接近饱和区时，变换太缓慢，导数趋于0，这种情况会造成信息丢失），这种现象称为饱和，从而无法完成深层网络的训练。而ReLU就不会有饱和倾向，不会有特别小的梯度出现。

第三，Relu会使一部分神经元的输出为0，这样就造成了网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生（以及一些人的生物解释balabala）。当然现在也有一些对relu的改进，比如prelu，random relu等，在不同的数据集上会有一些训练速度上或者准确率上的改进。

**12.为什么LSTM中既存在tanh和sigmoid，而不同意采用一样的。**

![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1512979821_580.jpg)

sigmoid 用在了各种gate上，产生0~1之间的值，这个一般只有sigmoid最直接了。 tanh 用在了状态和输出上，是对数据的处理，这个用其他激活函数或许也可以。

**13.如何解决RNN梯度消失和弥散的情况？**

为了解决梯度爆炸问题，Thomas Mikolov首先提出了一个简单的启发性的解决方案，就是当梯度大于一定阈值的的时候，将它截断为一个较小的数。具体如算法1所述： 算法：当梯度爆炸时截断梯度（伪代码）

![](http://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64153129974383405923.png)

下图可视化了梯度截断的效果。它展示了一个小的rnn（其中W为权值矩阵，b为bias项）的决策面。这个模型是一个一小段时间的rnn单元组成；实心箭头表明每步梯度下降的训练过程。当梯度下降过程中，模型的目标函数取得了较高的误差时，梯度将被送到远离决策面的位置。截断模型产生了一个虚线，它将误差梯度拉回到离原始梯度接近的位置。

![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1512979236_254.png)

梯度爆炸，梯度截断可视化 为了解决梯度弥散的问题，我们介绍了两种方法。第一种方法是将随机初始化![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1513007236_970.jpg)改为一个有关联的矩阵初始化。第二种方法是使用ReLU（Rectified Linear Units）代替sigmoid函数。ReLU的导数不是0就是1.因此，神经元的梯度将始终为1，而不会当梯度传播了一定时间之后变小。

**14.什么样的资料集不适合深度学习？**

1、数据集太小，数据样本不足时，深度学习相对其它机器学习算法，没有明显优势。

2、数据集没有局部相关特性，目前深度学习表现比较好的领域主要是图像／语音／自然语言处理等领域，这些领域的一个共性是局部相关性。图像中像素组成物体，语音信号中音位组合成单词，文本数据中单词组合成句子，这些特征元素的组合一旦被打乱，表示的含义同时也被改变。对于没有这样的局部相关性的数据集，不适于使用深度学习算法进行处理。举个例子：预测一个人的健康状况，相关的参数会有年龄、职业、收入、家庭状况等各种元素，将这些元素打乱，并不会影响相关的结果。

**15.如何解决梯度消失和梯度爆炸？**

（1）梯度消失： 根据链式法则，如果每一层神经元对上一层的输出的偏导乘上权重结果都小于1的话，那么即使这个结果是0.99，在经过足够多层传播之后，误差对输入层的偏导会趋于0 可以采用ReLU激活函数有效的解决梯度消失的情况，也可以用Batch Normalization解决这个问题。关于深度学习中 Batch Normalization为什么效果好？参见：https://www.zhihu.com/question/38102762

（2）梯度膨胀 ：根据链式法则，如果每一层神经元对上一层的输出的偏导乘上权重结果都大于1的话，在经过足够多层传播之后，误差对输入层的偏导会趋于无穷大 可以通过RELU激活函数来解决，或用Batch Normalization解决这个问题。
