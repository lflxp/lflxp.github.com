**16.CNN常用的几个简单模型**

见[基础网络](../2.基础网络/README.md)

**17.梯度爆炸会引发什么？**

在深度多层感知机网络中，梯度爆炸会引起网络不稳定，最好的结果是无法从训练数据中学习，而最坏的结果是出现无法再更新的 NaN 权重值。 梯度爆炸导致学习过程不稳定。—《深度学习》，2016. 在循环神经网络中，梯度爆炸会导致网络不稳定，无法利用训练数据学习，最好的结果是网络无法学习长的输入序列数据。

有很多方法可以解决梯度爆炸问题，本节列举了一些最佳实验方法。

1\. 重新设计网络模型 在深度神经网络中，梯度爆炸可以通过重新设计层数更少的网络来解决。 使用更小的批尺寸对网络训练也有好处。 在循环神经网络中，训练过程中在更少的先前时间步上进行更新（沿时间的截断反向传播，truncated Backpropagation through time）可以缓解梯度爆炸问题。

2\. 使用 ReLU 激活函数 在深度多层感知机神经网络中，梯度爆炸的发生可能是因为激活函数，如之前很流行的 Sigmoid 和 Tanh 函数。 使用 ReLU 激活函数可以减少梯度爆炸。采用 ReLU 激活函数是最适合隐藏层的新实践。

3\. 使用长短期记忆网络 在循环神经网络中，梯度爆炸的发生可能是因为某种网络的训练本身就存在不稳定性，如随时间的反向传播本质上将循环网络转换成深度多层感知机神经网络。 使用长短期记忆（LSTM）单元和相关的门类型神经元结构可以减少梯度爆炸问题。 采用 LSTM 单元是适合循环神经网络的序列预测的最新最好实践。

4\. 使用梯度截断（Gradient Clipping） 在非常深且批尺寸较大的多层感知机网络和输入序列较长的 LSTM 中，仍然有可能出现梯度爆炸。如果梯度爆炸仍然出现，你可以在训练过程中检查和限制梯度的大小。这就是梯度截断。 处理梯度爆炸有一个简单有效的解决方案：如果梯度超过阈值，就截断它们。 ——《Neural Network Methods in Natural Language Processing》，2017. 具体来说，检查误差梯度的值是否超过阈值，如果超过，则截断梯度，将梯度设置为阈值。 梯度截断可以一定程度上缓解梯度爆炸问题（梯度截断，即在执行梯度下降步骤之前将梯度设置为阈值）。 ——《深度学习》，2016. 在 Keras 深度学习库中，你可以在训练之前设置优化器上的 clipnorm 或 clipvalue 参数，来使用梯度截断。 默认值为 clipnorm=1.0 、clipvalue=0.5。详见：https://keras.io/optimizers/。

5\. 使用权重正则化（Weight Regularization） 如果梯度爆炸仍然存在，可以尝试另一种方法，即检查网络权重的大小，并惩罚产生较大权重值的损失函数。该过程被称为权重正则化，通常使用的是 L1 惩罚项（权重绝对值）或 L2 惩罚项（权重平方）。 对循环权重使用 L1 或 L2 惩罚项有助于缓解梯度爆炸。 ——On the difficulty of training recurrent neural networks，2013. 在 Keras 深度学习库中，你可以通过在层上设置 kernel\_regularizer 参数和使用 L1 或 L2 正则化项进行权重正则化。

**18.什么是RNN**

RNNs的目的使用来处理序列数据。在传统的神经网络模型中，是从输入层到隐含层再到输出层，层与层之间是全连接的，每层之间的节点是无连接的。但是这种普通的神经网络对于很多问题却无能无力。例如，你要预测句子的下一个单词是什么，一般需要用到前面的单词，因为一个句子中前后单词并不是独立的。 RNNs之所以称为循环神经网路，即一个序列当前的输出与前面的输出也有关。具体的表现形式为网络会对前面的信息进行记忆并应用于当前输出的计算中，即隐藏层之间的节点不再无连接而是有连接的，并且隐藏层的输入不仅包括输入层的输出还包括上一时刻隐藏层的输出。 理论上，RNNs能够对任何长度的序列数据进行处理。但是在实践中，为了降低复杂性往往假设当前的状态只与前面的几个状态相关，下图便是一个典型的RNNs：

![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1515688523_186.jpg)

RNNs包含输入单元(Input units)，输入集标记为{x0,x1,...,xt,xt+1,...}，而输出单元(Output units)的输出集则被标记为{y0,y1,...,yt,yt+1.,..}。RNNs还包含隐藏单元(Hidden units)，我们将其输出集标记为{s0,s1,...,st,st+1,...}，这些隐藏单元完成了最为主要的工作。你会发现，在图中：有一条单向流动的信息流是从输入单元到达隐藏单元的，与此同时另一条单向流动的信息流从隐藏单元到达输出单元。在某些情况下，RNNs会打破后者的限制，引导信息从输出单元返回隐藏单元，这些被称为“Back Projections”，并且隐藏层的输入还包括上一隐藏层的状态，即隐藏层内的节点可以自连也可以互连。

![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1515688554_151.png)

上图将循环神经网络进行展开成一个全神经网络。例如，对一个包含5个单词的语句，那么展开的网络便是一个五层的神经网络，每一层代表一个单词。对于该网络的计算过程如下： 1. xt表示第t,t=1,2,3...步(step)的输入。比如，x1为第二个词的one-hot向量(根据上图，x0为第一个词)； 2. st为隐藏层的第t步的状态，它是网络的记忆单元。 st根据当前输入层的输出与上一步隐藏层的状态进行计算。st=f(Uxt+Wst−1)，其中f一般是非线性的激活函数，如tanh或ReLU，在计算s0时，即第一个单词的隐藏层状态，需要用到s−1，但是其并不存在，在实现中一般置为0向量； 3. ot是第t步的输出，如下个单词的向量表示，ot=softmax(Vst).

**19.什么是LSTM网络？**

Long Short Term 网络—— 一般就叫做 LSTM ——是一种 RNN 特殊的类型，可以学习长期依赖信息。如@寒小阳所说：LSTM和基线RNN并没有特别大的结构不同，但是它们用了不同的函数来计算隐状态。LSTM的“记忆”我们叫做细胞/cells，你可以直接把它们想做黑盒，这个黑盒的输入为前状态ht−1和当前输入xt。这些“细胞”会决定哪些之前的信息和状态需要保留/记住，而哪些要被抹去。实际的应用中发现，这种方式可以有效地保存很长时间之前的关联信息。 LSTM 由Hochreiter & Schmidhuber (1997)提出，并在近期被Alex Graves进行了改良和推广。在很多问题，LSTM 都取得相当巨大的成功，并得到了广泛的使用。 LSTM 通过刻意的设计来避免长期依赖问题。记住长期的信息在实践中是 LSTM 的默认行为，而非需要付出很大代价才能获得的能力！ 所有 RNN 都具有一种重复神经网络模块的链式的形式。在标准的 RNN 中，这个重复的模块只有一个非常简单的结构，例如一个 tanh 层。

![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1515857524_689.png)

标准 RNN 中的重复模块包含单一的层 LSTM 同样是这样的结构，但是重复的模块拥有一个不同的结构。不同于 单一神经网络层，这里是有四个，以一种非常特殊的方式进行交互。

![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1515857583_596.png) LSTM 中的重复模块包含四个交互的层 不必担心这里的细节。我们会一步一步地剖析 LSTM 解析图。现在，我们先来熟悉一下图中使用的各种元素的图标。

![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1515857610_673.png)

LSTM 中的图标 在上面的图例中，每一条黑线传输着一整个向量，从一个节点的输出到其他节点的输入。粉色的圈代表 pointwise 的操作，诸如向量的和，而黄色的矩阵就是学习到的神经网络层。合在一起的线表示向量的连接，分开的线表示内容被复制，然后分发到不同的位置。 四、LSTM 的核心思想 LSTM 的关键就是细胞状态，水平线在图上方贯穿运行。 细胞状态类似于传送带。直接在整个链上运行，只有一些少量的线性交互。信息在上面流传保持不变会很容易。

![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1515857686_304.png)

LSTM 有通过精心设计的称作为“门”的结构来去除或者增加信息到细胞状态的能力。门是一种让信息选择式通过的方法。他们包含一个 sigmoid 神经网络层和一个 pointwise 乘法操作。

![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1515857715_135.png)

Sigmoid 层输出 0 到 1 之间的数值，描述每个部分有多少量可以通过。0 代表“不许任何量通过”，1 就指“允许任意量通过”！ LSTM 拥有三个门，来保护和控制细胞状态。 在我们 LSTM 中的第一步是决定我们会从细胞状态中丢弃什么信息。这个决定通过一个称为忘记门层完成。该门会读取 h\_{t-1} 和 x\_t，输出一个在 0 到 1 之间的数值给每个在细胞状态 C\_{t-1} 中的数字。1 表示“完全保留”，0 表示“完全舍弃”。 让我们回到语言模型的例子中来基于已经看到的预测下一个词。在这个问题中，细胞状态可能包含当前主语的性别，因此正确的代词可以被选择出来。当我们看到新的主语，我们希望忘记旧的主语。

![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1515857798_931.png)

决定丢弃信息 下一步是确定什么样的新信息被存放在细胞状态中。这里包含两个部分。第一，sigmoid 层称 “输入门层” 决定什么值我们将要更新。然后，一个 tanh 层创建一个新的候选值向量，\\tilde{C}\_t，会被加入到状态中。下一步，我们会讲这两个信息来产生对状态的更新。 在我们语言模型的例子中，我们希望增加新的主语的性别到细胞状态中，来替代旧的需要忘记的主语。

![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1515857956_610.png)

确定更新的信息 现在是更新旧细胞状态的时间了，C\_{t-1} 更新为 C\_t。前面的步骤已经决定了将会做什么，我们现在就是实际去完成。 我们把旧状态与 f\_t 相乘，丢弃掉我们确定需要丢弃的信息。接着加上 i\_t \* \\tilde{C}\_t。这就是新的候选值，根据我们决定更新每个状态的程度进行变化。 在语言模型的例子中，这就是我们实际根据前面确定的目标，丢弃旧代词的性别信息并添加新的信息的地方。

![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1515858026_294.png)

更新细胞状态 最终，我们需要确定输出什么值。这个输出将会基于我们的细胞状态，但是也是一个过滤后的版本。首先，我们运行一个 sigmoid 层来确定细胞状态的哪个部分将输出出去。接着，我们把细胞状态通过 tanh 进行处理（得到一个在 -1 到 1 之间的值）并将它和 sigmoid 门的输出相乘，最终我们仅仅会输出我们确定输出的那部分。 在语言模型的例子中，因为他就看到了一个 代词，可能需要输出与一个 动词 相关的信息。例如，可能输出是否代词是单数还是负数，这样如果是动词的话，我们也知道动词需要进行的词形变化。

![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1515858060_635.png)

**20.详细说说CNN工作原理**

1 人工神经网络

1.1 神经元 神经网络由大量的神经元相互连接而成。每个神经元接受线性组合的输入后，最开始只是简单的线性加权，后来给每个神经元加上了非线性的激活函数，从而进行非线性变换后输出。每两个神经元之间的连接代表加权值，称之为权重（weight）。不同的权重和激活函数，则会导致神经网络不同的输出。 举个手写识别的例子，给定一个未知数字，让神经网络识别是什么数字。此时的神经网络的输入由一组被输入图像的像素所激活的输入神经元所定义。在通过非线性激活函数进行非线性变换后，神经元被激活然后被传递到其他神经元。重复这一过程，直到最后一个输出神经元被激活。从而识别当前数字是什么字。 神经网络的每个神经元如下

![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1516764270_334.png)

基本wx + b的形式，其中 x1、x2表示输入向量 w1、w2为权重，几个输入则意味着有几个权重，即每个输入都被赋予一个权重 b为偏置bias g(z) 为激活函数 a 为输出 如果只是上面这样一说，估计以前没接触过的十有八九又必定迷糊了。事实上，上述简单模型可以追溯到20世纪50/60年代的感知器，可以把感知器理解为一个根据不同因素、以及各个因素的重要性程度而做决策的模型。 举个例子，这周末北京有一草莓音乐节，那去不去呢？决定你是否去有二个因素，这二个因素可以对应二个输入，分别用x1、x2表示。此外，这二个因素对做决策的影响程度不一样，各自的影响程度用权重w1、w2表示。一般来说，音乐节的演唱嘉宾会非常影响你去不去，唱得好的前提下 即便没人陪同都可忍受，但如果唱得不好还不如你上台唱呢。所以，我们可以如下表示： x1：是否有喜欢的演唱嘉宾。x1 = 1 你喜欢这些嘉宾，x1 = 0 你不喜欢这些嘉宾。嘉宾因素的权重w1 = 7 x2：是否有人陪你同去。x2 = 1 有人陪你同去，x2 = 0 没人陪你同去。是否有人陪同的权重w2 = 3。 这样，咱们的决策模型便建立起来了：g(z) = g(w1\*x1 + w2\*x2 + b )，g表示激活函数，这里的b可以理解成 为更好达到目标而做调整的偏置项。 一开始为了简单，人们把激活函数定义成一个线性函数，即对于结果做一个线性变化，比如一个简单的线性激活函数是g(z) = z，输出都是输入的线性变换。后来实际应用中发现，线性激活函数太过局限，于是人们引入了非线性激活函数。

1.2 激活函数 常用的非线性激活函数有sigmoid、tanh、relu等等，前两者sigmoid/tanh比较常见于全连接层，后者relu常见于卷积层。这里先简要介绍下最基础的sigmoid函数（btw，在本博客中SVM那篇文章开头有提过）。 sigmoid的函数表达式如下

![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1516764421_303.png)

其中z是一个线性组合，比如z可以等于：b + w1\*x1 + w2\*x2。通过代入很大的正数或很小的负数到g(z)函数中可知，其结果趋近于0或1。 因此，sigmoid函数g(z)的图形表示如下（ 横轴表示定义域z，纵轴表示值域g(z) ）：

![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1516764454_682.jpg)

也就是说，sigmoid函数的功能是相当于把一个实数压缩至0到1之间。当z是非常大的正数时，g(z)会趋近于1，而z是非常小的负数时，则g(z)会趋近于0。 压缩至0到1有何用处呢？用处是这样一来便可以把激活函数看作一种“分类的概率”，比如激活函数的输出为0.9的话便可以解释为90%的概率为正样本。 举个例子，如下图（图引自Stanford机器学习公开课）

![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1516764516_976.png)

z = b + w1\*x1 + w2\*x2，其中b为偏置项 假定取-30，w1、w2都取为20

![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1516764562_932.jpg)

如果x1 = 0 x2 = 0，则z = -30，g(z) = 1/( 1 + e^-z )趋近于0。此外，从上图sigmoid函数的图形上也可以看出，当z=-30的时候，g(z)的值趋近于0 如果x1 = 0 x2 = 1，或x1 =1 x2 = 0，则z = b + w1\*x1 + w2\*x2 = -30 + 20 = -10，同样，g(z)的值趋近于0 如果x1 = 1 x2 = 1，则z = b + w1\*x1 + w2\*x2 = -30 + 20\*1 + 20\*1 = 10，此时，g(z)趋近于1。 换言之，只有和都取1的时候，g(z)→1，判定为正样本；或取0的时候，g(z)→0，判定为负样本，如此达到分类的目的。

1.3 神经网络 将下图的这种单个神经元

![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1516764664_457.png)

组织在一起，便形成了神经网络。下图便是一个三层神经网络结构

![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1516764679_794.png)

上图中最左边的原始输入信息称之为输入层，最右边的神经元称之为输出层（上图中输出层只有一个神经元），中间的叫隐藏层。 啥叫输入层、输出层、隐藏层呢？ 输入层（Input layer），众多神经元（Neuron）接受大量非线形输入讯息。输入的讯息称为输入向量。 输出层（Output layer），讯息在神经元链接中传输、分析、权衡，形成输出结果。输出的讯息称为输出向量。 隐藏层（Hidden layer），简称“隐层”，是输入层和输出层之间众多神经元和链接组成的各个层面。如果有多个隐藏层，则意味着多个激活函数。 同时，每一层都可能由单个或多个神经元组成，每一层的输出将会作为下一层的输入数据。比如下图中间隐藏层来说，隐藏层的3个神经元a1、a2、a3皆各自接受来自多个不同权重的输入（因为有x1、x2、x3这三个输入，所以a1 a2 a3都会接受x1 x2 x3各自分别赋予的权重，即几个输入则几个权重），接着，a1、a2、a3又在自身各自不同权重的影响下 成为的输出层的输入，最终由输出层输出最终结果。

![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1516764767_233.png)

上图（图引自Stanford机器学习公开课）中

![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1516764777_706.png)表示第j层第i个单元的激活函数/神经元 ![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1516764783_937.png)

表示从第j层映射到第j+1层的控制函数的权重矩阵 此外，输入层和隐藏层都存在一个偏置（bias unit)，所以上图中也增加了偏置项：x0、a0。针对上图，有如下公式

![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1516770180_891.png)

此外，上文中讲的都是一层隐藏层，但实际中也有多层隐藏层的，即输入层和输出层中间夹着数层隐藏层，层和层之间是全连接的结构，同一层的神经元之间没有连接。

![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1516764808_830.jpg)

2 卷积神经网络之层级结构 cs231n课程里给出了卷积神经网络各个层级结构，如下图

![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1516770378_513.jpg)

上图中CNN要做的事情是：给定一张图片，是车还是马未知，是什么车也未知，现在需要模型判断这张图片里具体是一个什么东西，总之输出一个结果：如果是车 那是什么车 所以 最左边是数据输入层，对数据做一些处理，比如去均值（把输入数据各个维度都中心化为0，避免数据过多偏差，影响训练效果）、归一化（把所有的数据都归一到同样的范围）、PCA/白化等等。CNN只对训练集做“去均值”这一步。 中间是 CONV：卷积计算层，线性乘积 求和。 RELU：激励层，上文2.2节中有提到：ReLU是激活函数的一种。 POOL：池化层，简言之，即取区域平均或最大。 最右边是 FC：全连接层 这几个部分中，卷积计算层是CNN的核心，下文将重点阐述。

3 CNN之卷积计算层 3.1 CNN怎么进行识别 当我们给定一个"X"的图案，计算机怎么识别这个图案就是“X”呢？一个可能的办法就是计算机存储一张标准的“X”图案，然后把需要识别的未知图案跟标准"X"图案进行比对，如果二者一致，则判定未知图案即是一个"X"图案。 而且即便未知图案可能有一些平移或稍稍变形，依然能辨别出它是一个X图案。如此，CNN是把未知图案和标准X图案一个局部一个局部的对比，如下图所示

![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1516764841_515.png)

而未知图案的局部和标准X图案的局部一个一个比对时的计算过程，便是卷积操作。卷积计算结果为1表示匹配，否则不匹配。 接下来，我们来了解下什么是卷积操作。 3.2 什么是卷积 对图像（不同的数据窗口数据）和滤波矩阵（一组固定的权重：因为每个神经元的多个权重固定，所以又可以看做一个恒定的滤波器filter）做内积（逐个元素相乘再求和）的操作就是所谓的『卷积』操作，也是卷积神经网络的名字来源。 非严格意义上来讲，下图中红框框起来的部分便可以理解为一个滤波器，即带着一组固定权重的神经元。多个滤波器叠加便成了卷积层。

![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1516764868_951.png)

OK，举个具体的例子。比如下图中，图中左边部分是原始输入数据，图中中间部分是滤波器filter，图中右边是输出的新的二维数据。

![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1516764882_762.png)

分解下上图 ![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1516764898_144.png)对应位置上是数字先相乘后相加 ![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1516764912_847.png) = ![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1516764919_694.png)

中间滤波器filter与数据窗口做内积，其具体计算过程则是：

4\*0 + 0\*0 + 0\*0 + 0\*0 + 0\*1 + 0\*1 + 0\*0 + 0\*1 + -4\*2 = -8 3.3

图像上的卷积 在下图对应的计算过程中，输入是一定区域大小(width\*height)的数据，和滤波器filter（带着一组固定权重的神经元）做内积后等到新的二维数据。 具体来说，左边是图像输入，中间部分就是滤波器filter（带着一组固定权重的神经元），不同的滤波器filter会得到不同的输出数据，比如颜色深浅、轮廓。相当于如果想提取图像的不同特征，则用不同的滤波器filter，提取想要的关于图像的特定信息：颜色深浅或轮廓。 如下图所示

![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1516764936_540.png)

3.4 GIF动态卷积图 在CNN中，滤波器filter（带着一组固定权重的神经元）对局部输入数据进行卷积计算。每计算完一个数据窗口内的局部数据后，数据窗口不断平移滑动，直到计算完所有数据。这个过程中，有这么几个参数：

a. 深度depth：神经元个数，决定输出的depth厚度。同时代表滤波器个数。

b. 步长stride：决定滑动多少步可以到边缘。

c. 填充值zero-padding：在外围边缘补充若干圈0，方便从初始位置以步长为单位可以刚好滑倒末尾位置，通俗地讲就是为了总长能被步长整除。

![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1516764965_191.png)
